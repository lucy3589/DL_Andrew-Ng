1.注意在参数theta更新的时候，带入的时候谁是原来的参数，谁是之后的参数(只有在y的计算里面theta的带入才是原来的)，
  在进行所有参数theta更新的时候，是同步进行的，也就是theta_1和theta_0的更新里面y值得带入都是最原始的哪一组参数，而不是更新之后的。
2.要进行特征归一化的操作，特征归一化的作用：
  各特征之间的大小范围一致，才能使用距离度量等算法
  加速梯度下降算法的收敛
  在SVM算法中，一致化的特征能加速寻找支持向量的时间
  不同的机器学习算法，能接受的输入数值范围不一样
3.特征归一化的主要方法：
  当我们需要将特征值都归一化为某个范围[a,b]时，选MinMaxScaler
  当我们需要归一化后的特征值均值为0，标准差为1，选StandardScaler
  (1)MinMaxScaler：
  (2)
4.正规方程和梯度下降
  梯度下降                             正规方程
  缺点：需要选择学习率          优点：不要选择学习率
    需要迭代                        不需要迭代 
  有可能需要进行特征缩放           不需要进行特征缩放(就是根据数学公式自己
                                  求解出来的)，但是它的运算涉及到方阵的乘法，所
                                    以计算量很大(当特征的个数很多的时候)
  优点：                        ：当特征少的时时候(一般不超过1w)，使用它比较
  当特征比较多的时候                   方便
  (一般上万的时候)， 
  需要适应梯度下降算法 

5.正规方程里面的求解
  如果X‘X是不可逆的话，对应在octave里面也有相应的函数去计算出来它的‘逆’的值，是函数pinv()，这个函数不管是否可逆，
  都可以计算出来。但是函数inv()只能计算出来可逆矩阵的逆
6.X'X是不可逆的原因：
  (1)特征之间具有线性关系
  比如：x1是x2的某种线性运算得到的
  所以先进行观察，看看是否有重复的特征(比如某种存在线性搞关系的特征)，若有则进行删除
  (2)特征太多(也就是数据量比较少)(m<=n)
  若特征太多，看看是否在不影响结果的前提下，删除一些特征 或者 使用正规方程的解法
7.正规方程的解法：
  Rss = \sum_{i=1}^{n}({y_i-y_i'} )^2= \sum_{i=1}^{n}({y_i-h(x_i)} )^2 = (y-h(X))^T*(y-h(X))
  见网络的搜索：https://zhuanlan.zhihu.com/p/22474562
